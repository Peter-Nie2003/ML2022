{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34954,"databundleVersionId":3300998,"sourceType":"competition"}],"dockerImageVersionId":30171,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# If you want to access the version you have already modified, click \"Edit\"\n# If you want to access the original sample code, click \"...\", then click \"Copy & Edit Notebook\"","metadata":{}},{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":19.351342,"end_time":"2022-02-23T10:03:06.247288","exception":false,"start_time":"2022-02-23T10:02:46.895946","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:01.679031Z","iopub.execute_input":"2024-01-20T15:55:01.679321Z","iopub.status.idle":"2024-01-20T15:55:08.804723Z","shell.execute_reply.started":"2024-01-20T15:55:01.679233Z","shell.execute_reply":"2024-01-20T15:55:08.804068Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"papermill":{"duration":0.0189,"end_time":"2022-02-23T10:03:06.279758","exception":false,"start_time":"2022-02-23T10:03:06.260858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:08.806145Z","iopub.execute_input":"2024-01-20T15:55:08.806352Z","iopub.status.idle":"2024-01-20T15:55:08.811658Z","shell.execute_reply.started":"2024-01-20T15:55:08.806327Z","shell.execute_reply":"2024-01-20T15:55:08.810498Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"papermill":{"duration":1.654263,"end_time":"2022-02-23T10:03:07.947242","exception":false,"start_time":"2022-02-23T10:03:06.292979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:08.812619Z","iopub.execute_input":"2024-01-20T15:55:08.813064Z","iopub.status.idle":"2024-01-20T15:55:10.507684Z","shell.execute_reply.started":"2024-01-20T15:55:08.813021Z","shell.execute_reply":"2024-01-20T15:55:10.507050Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"papermill":{"duration":0.078771,"end_time":"2022-02-23T10:03:08.039428","exception":false,"start_time":"2022-02-23T10:03:07.960657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.509635Z","iopub.execute_input":"2024-01-20T15:55:10.509868Z","iopub.status.idle":"2024-01-20T15:55:10.564907Z","shell.execute_reply.started":"2024-01-20T15:55:10.509840Z","shell.execute_reply":"2024-01-20T15:55:10.564102Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Transforms**\nTorchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n\nPlease refer to PyTorch official website for details about different transforms.","metadata":{"papermill":{"duration":0.01289,"end_time":"2022-02-23T10:03:08.065357","exception":false,"start_time":"2022-02-23T10:03:08.052467","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    transforms.RandomRotation(15),\n    transforms.RandomRotation(30),\n    transforms.ColorJitter(brightness=1),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.5),\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])\n","metadata":{"papermill":{"duration":0.021406,"end_time":"2022-02-23T10:03:08.099437","exception":false,"start_time":"2022-02-23T10:03:08.078031","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.566084Z","iopub.execute_input":"2024-01-20T15:55:10.566318Z","iopub.status.idle":"2024-01-20T15:55:10.575173Z","shell.execute_reply.started":"2024-01-20T15:55:10.566285Z","shell.execute_reply":"2024-01-20T15:55:10.574511Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## **Datasets**\nThe data is labelled by the name, so we load images and label while calling '__getitem__'","metadata":{"papermill":{"duration":0.012739,"end_time":"2022-02-23T10:03:08.125181","exception":false,"start_time":"2022-02-23T10:03:08.112442","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label\n\n","metadata":{"papermill":{"duration":0.023022,"end_time":"2022-02-23T10:03:08.160912","exception":false,"start_time":"2022-02-23T10:03:08.13789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.576162Z","iopub.execute_input":"2024-01-20T15:55:10.576365Z","iopub.status.idle":"2024-01-20T15:55:10.587489Z","shell.execute_reply.started":"2024-01-20T15:55:10.576339Z","shell.execute_reply":"2024-01-20T15:55:10.586809Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn_layer1=nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  \n            nn.BatchNorm2d(64),\n            \n        )\n        \n        self.cnn_layer2=nn.Sequential(\n            nn.Conv2d(64, 64, 3, 1, 1), \n            nn.BatchNorm2d(64),\n            \n        )\n        \n        self.cnn_layer3=nn.Sequential(\n            nn.Conv2d(64, 128, 3, 1, 1), \n            nn.BatchNorm2d(128),\n           \n        )\n        \n        self.cnn_layer4=nn.Sequential(\n            nn.Conv2d(128,128, 3, 1, 1), \n            nn.BatchNorm2d(128),\n           \n        )\n        \n        self.cnn_layer5=nn.Sequential(\n            nn.Conv2d(128, 256, 3, 1, 1), \n            nn.BatchNorm2d(256),\n           \n        )\n        self.cnn_layer6=nn.Sequential(\n            nn.Conv2d(256, 256, 3, 1, 1), \n            nn.BatchNorm2d(256),\n           \n        )\n        self.relu=nn.Sequential(\n            nn.ReLU(),\n        )\n        \n        self.maxpool=nn.Sequential(\n            nn.MaxPool2d(2,2,0),\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(256*8*8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(256, 11)\n        )\n\n    def forward(self, x):\n        x1=self.cnn_layer1(x)\n        x1=self.relu(x1)\n        x1=self.maxpool(x1)\n        residual=x1\n        x2=self.cnn_layer2(x1)\n        x2=self.relu(residual+x2)\n        x2=self.maxpool(x2)\n        x3=self.cnn_layer3(x2)\n        x3=self.relu(x3)\n        x3=self.maxpool(x3)\n        residual=x3\n        x4=self.cnn_layer4(x3)\n        x4=self.relu(residual+x4)\n        x5=self.cnn_layer5(x4)\n        x5=self.relu(x5)\n        residual=x5\n        x6=self.cnn_layer6(x5)\n        x6=self.relu(residual+x6)\n        x6=self.maxpool(x6)\n        xout=x6.flatten(1)\n        return self.fc(xout)\n        \n        \n        \n        ","metadata":{"papermill":{"duration":0.0258,"end_time":"2022-02-23T10:03:08.199437","exception":false,"start_time":"2022-02-23T10:03:08.173637","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.588663Z","iopub.execute_input":"2024-01-20T15:55:10.589149Z","iopub.status.idle":"2024-01-20T15:55:10.605662Z","shell.execute_reply.started":"2024-01-20T15:55:10.589111Z","shell.execute_reply":"2024-01-20T15:55:10.604831Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def trainer_k_folds(config, dataset_dir, batch_size, train_tfm, test_tfm, devices):\n    train_dir = os.path.join(dataset_dir,\"training\")\n    val_dir = os.path.join(dataset_dir,\"validation\")\n    train_files = [os.path.join(train_dir, x) for x in os.listdir(train_dir) if x.endswith('.jpg')]\n    val_files = [os.path.join(val_dir, x) for x in os.listdir(val_dir) if x.endswith('.jpg')]\n    total_files = np.array(train_files + val_files)\n    random.shuffle(total_files)\n    num_folds = config['num_folds']   \n    train_folds = np.array_split(np.arange(len(total_files)), num_folds)\n    train_folds = np.array(train_folds, dtype=object) # 防止因为数组维度不整齐而报错\n        \n    for i in range(num_folds):\n        print(f'\\n\\nStarting Fold: {i} ********************************************')  \n        train_data = total_files[np.concatenate(np.delete(train_folds, i)) ] \n        val_data = total_files[train_folds[i]]        \n    \n        train_set = FoodDataset(tfm=train_tfm, files=train_data)\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last = True)    \n        valid_set = FoodDataset(tfm=test_tfm, files=val_data)\n        valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last = True)\n        print('训练集总长度是 {:d}, batch数量是 {:.2f}'.format(len(train_set), len(train_set)/ batch_size))\n        print('验证集总长度是 {:d}, batch数量是 {:.2f}'.format(len(valid_set), len(valid_set)/ batch_size))\n        \n        tep = config['model_path']\n        config['model_path'] += f\"Fold_{i}_best\"\n        config['best_acc'] = 0.0\n        model = Classifier().to(devices[0])\n        # model.load_state_dict(torch.load('models/foldmodel0.0001')) 提前训练几个epoch，可能加快后面每一个模型的训练\n        trainer(train_loader, valid_loader, model, config, devices)\n        config['best_accs'].append(config['best_acc'])\n        config['model_path'] = tep\n\n\nbatch_size = 512\n_dataset_dir = \"../input/ml2022spring-hw3b/food11\"\n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = FoodDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\n# fix random seed\nseed = 0                        # random seed\nsame_seeds(seed)\n \nconfig = {\n    # training prarameters\n    'num_epoch': 300,             # the number of training epoch\n    'learning_rate': 5e-4,          # learning rate \n    'weight_decay': 1e-4, \n    'best_acc': 0.0,\n    'T_0': 16,\n    'T_mult': 1,    \n    'eta_min_ratio':50,\n    'patience': 32, \n    'num_folds':5,\n    'show_num': 1,\n    'best_accs': []\n}\nconfig['model_path'] = './models22/model' + str(config['learning_rate']) # the path where the checkpoint will be saved\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = Classifier().to(device)\n\ntrainer_k_folds(config, dataset_dir, batch_size, train_tfm, test_tfm, devices)\n","metadata":{"papermill":{"duration":0.054295,"end_time":"2022-02-23T10:03:08.266338","exception":false,"start_time":"2022-02-23T10:03:08.212043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.606672Z","iopub.execute_input":"2024-01-20T15:55:10.606925Z","iopub.status.idle":"2024-01-20T15:55:10.774445Z","shell.execute_reply.started":"2024-01-20T15:55:10.606898Z","shell.execute_reply":"2024-01-20T15:55:10.773275Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"One ../input/ml2022spring-hw3b/food11/training sample ../input/ml2022spring-hw3b/food11/training/0_0.jpg\nOne ../input/ml2022spring-hw3b/food11/validation sample ../input/ml2022spring-hw3b/food11/validation/0_0.jpg\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3476335689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# fix random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m                        \u001b[0;31m# random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0msame_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m config = {\n","\u001b[0;31mNameError\u001b[0m: name 'same_seeds' is not defined"],"ename":"NameError","evalue":"name 'same_seeds' is not defined","output_type":"error"}]},{"cell_type":"code","source":"test_set = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loaders = []\nfor i in range(5):\n    test_set_i = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=train_tfm)\n    test_loader_i = DataLoader(test_set_i, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loaders.append(test_loader_i)\n    ","metadata":{"papermill":{"duration":0.493644,"end_time":"2022-02-23T19:10:19.985992","exception":false,"start_time":"2022-02-23T19:10:19.492348","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.775461Z","iopub.status.idle":"2024-01-20T15:55:10.775914Z","shell.execute_reply.started":"2024-01-20T15:55:10.775653Z","shell.execute_reply":"2024-01-20T15:55:10.775679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing and generate prediction CSV","metadata":{"papermill":{"duration":0.498773,"end_time":"2022-02-23T19:10:20.961802","exception":false,"start_time":"2022-02-23T19:10:20.463029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\ndef trainer(train_loader, val_loader, model, config, devices):  \n    \n    criterion = nn.CrossEntropyLoss() \n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n                T_0=config['T_0'], T_mult=config['T_mult'], \n                eta_min=config['learning_rate']/config['eta_min_ratio'])\n    n_epochs, patience = config['num_epoch'], config['patience']\n    num_batches = len(train_loader)\n    show_batches = num_batches // config['show_num']\n    \n    if not os.path.isdir('./' + config['model_path'].split('/')[1]):\n        os.mkdir('./' + config['model_path'].split('/')[1]) # Create directory of saving models.\n    legend = ['train loss', 'train acc']\n    \n    if val_loader is not None:\n        legend.append('valid loss')  \n        legend.append('valid acc')  \n    animator = d2l.Animator(xlabel='epoch', xlim=[0, n_epochs], legend=legend)       \n        \n    for epoch in range(n_epochs):\n        train_acc, train_loss = 0.0, 0.0        \n        \n        # training\n        model.train() # set the model to training mode\n        for i, (data, labels) in enumerate(train_loader):\n            data, labels = data.to(devices[0]), labels.to(devices[0])         \n \n            optimizer.zero_grad() \n            outputs = model(data)             \n            \n            loss = criterion(outputs, labels)\n            loss.backward() \n            optimizer.step() \n \n            _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n            train_acc += (train_pred.detach() == labels.detach()).sum().item()\n            train_loss += loss.item()            \n        \n            if (i + 1) % show_batches == 0:\n                train_acc = train_acc / show_batches / len(data)\n                train_loss = train_loss / show_batches\n                print('train_acc {:.3f}, train_loss {:.3f}'.format(train_acc, train_loss))\n                animator.add(epoch  + (i + 1) / num_batches, (train_loss, train_acc, None, None)) \n                train_acc, train_loss = 0.0, 0.0               \n                \n        scheduler.step()\n        # validation\n        if val_loader != None:\n            model.eval() # set the model to evaluation mode\n            val_acc, val_loss = 0.0, 0.0  \n            with torch.no_grad():\n                for i, (data, labels) in enumerate(val_loader):\n                    data, labels = data.to(devices[0]), labels.to(devices[0])\n                    outputs = model(data)\n                                        \n                    loss = criterion(outputs, labels) \n \n                    _, val_pred = torch.max(outputs, 1) \n                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n                    val_loss += loss.item()                \n \n                val_acc = val_acc / len(val_loader) / len(data)\n                val_loss = val_loss / len(val_loader)\n                print('val_acc {:.3f}, val_loss {:.3f} '.format(val_acc, val_loss))\n                animator.add(epoch + 1, (None, None, val_loss, val_acc))\n                \n                # if the model improves, save a checkpoint at this epoch\n                if val_acc > config['best_acc']:\n                    config['best_acc'] = val_acc\n                    torch.save(model.state_dict(),  config['model_path'])\n                    # print('saving model with acc {:.3f}'.format(best_acc / len(val_loader) / len(labels)))\n                    stale = 0\n                else:\n                    stale += 1\n                    if stale > patience:\n                        print(f\"No improvment {patience} consecutive epochs, early stopping\")\n                        break\n \n    # if not validating, save the last epoch\n    if val_loader == None:\n        torch.save(model.state_dict(), config['model_path'])\n        # print('saving model at last epoch')      \n","metadata":{"papermill":{"duration":32830.720158,"end_time":"2022-02-23T19:10:19.001001","exception":false,"start_time":"2022-02-23T10:03:08.280843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.777561Z","iopub.status.idle":"2024-01-20T15:55:10.777923Z","shell.execute_reply.started":"2024-01-20T15:55:10.777708Z","shell.execute_reply":"2024-01-20T15:55:10.777730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i in (config['num_folds']):\n    models[i]=Classifier().to(device)\n    models[i].load_state_dict(torch.load( config['model_path'] + f\"Fold_{i}_best\"))\n    models[i].eval()\n    \nwith torch.no_grad():\n    for data, _ in test_loader:\n        batch_preds = [] \n        for model_best in models:\n            batch_preds.append(model_best(data.to(devices[0])).cpu().data.numpy())\n        batch_preds = sum(batch_preds)\n        preds[0].extend(batch_preds.squeeze().tolist())    \n    \n    for i, loader in enumerate(test_loaders):\n        for data, _ in loader:\n            batch_preds = []\n            for model_best in models:\n                batch_preds.append(model_best(data.to(devices[0])).cpu().data.numpy())\n            batch_preds = sum(batch_preds)\n            preds[i+1].extend(batch_preds.squeeze().tolist())\n            \n    preds_np = np.array(preds, dtype=object)\n    print(preds_np.shape)\n    bb = 0.6* preds_np[0] + 0.1 * preds_np[1] + 0.1 * preds_np[2] + 0.1 * preds_np[3] + 0.1 * preds_np[4] + 0.1 * preds_np[5]\n    print(bb.shape)\n    prediction = np.argmax(bb, axis=1)\n\n","metadata":{"papermill":{"duration":49.157727,"end_time":"2022-02-23T19:11:10.61523","exception":false,"start_time":"2022-02-23T19:10:21.457503","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.778817Z","iopub.status.idle":"2024-01-20T15:55:10.779148Z","shell.execute_reply.started":"2024-01-20T15:55:10.778977Z","shell.execute_reply":"2024-01-20T15:55:10.778999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"papermill":{"duration":0.554276,"end_time":"2022-02-23T19:11:11.870035","exception":false,"start_time":"2022-02-23T19:11:11.315759","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-20T15:55:10.780125Z","iopub.status.idle":"2024-01-20T15:55:10.780431Z","shell.execute_reply.started":"2024-01-20T15:55:10.780260Z","shell.execute_reply":"2024-01-20T15:55:10.780285Z"},"trusted":true},"execution_count":null,"outputs":[]}]}