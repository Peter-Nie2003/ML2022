{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34954,"databundleVersionId":3300998,"sourceType":"competition"}],"dockerImageVersionId":30171,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# If you want to access the version you have already modified, click \"Edit\"\n# If you want to access the original sample code, click \"...\", then click \"Copy & Edit Notebook\"","metadata":{}},{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":19.351342,"end_time":"2022-02-23T10:03:06.247288","exception":false,"start_time":"2022-02-23T10:02:46.895946","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:04.661575Z","iopub.execute_input":"2024-01-12T22:49:04.661923Z","iopub.status.idle":"2024-01-12T22:49:17.680609Z","shell.execute_reply.started":"2024-01-12T22:49:04.661833Z","shell.execute_reply":"2024-01-12T22:49:17.679776Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"papermill":{"duration":0.0189,"end_time":"2022-02-23T10:03:06.279758","exception":false,"start_time":"2022-02-23T10:03:06.260858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:17.684933Z","iopub.execute_input":"2024-01-12T22:49:17.685186Z","iopub.status.idle":"2024-01-12T22:49:17.689174Z","shell.execute_reply.started":"2024-01-12T22:49:17.685156Z","shell.execute_reply":"2024-01-12T22:49:17.688287Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"papermill":{"duration":1.654263,"end_time":"2022-02-23T10:03:07.947242","exception":false,"start_time":"2022-02-23T10:03:06.292979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:17.690375Z","iopub.execute_input":"2024-01-12T22:49:17.691311Z","iopub.status.idle":"2024-01-12T22:49:19.343079Z","shell.execute_reply.started":"2024-01-12T22:49:17.691273Z","shell.execute_reply":"2024-01-12T22:49:19.342272Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"papermill":{"duration":0.078771,"end_time":"2022-02-23T10:03:08.039428","exception":false,"start_time":"2022-02-23T10:03:07.960657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:19.344248Z","iopub.execute_input":"2024-01-12T22:49:19.344503Z","iopub.status.idle":"2024-01-12T22:49:19.394700Z","shell.execute_reply.started":"2024-01-12T22:49:19.344473Z","shell.execute_reply":"2024-01-12T22:49:19.393892Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Transforms**\nTorchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n\nPlease refer to PyTorch official website for details about different transforms.","metadata":{"papermill":{"duration":0.01289,"end_time":"2022-02-23T10:03:08.065357","exception":false,"start_time":"2022-02-23T10:03:08.052467","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    transforms.RandomRotation(15),\n    transforms.RandomRotation(30),\n    transforms.ColorJitter(brightness=1),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.5),\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])\n","metadata":{"papermill":{"duration":0.021406,"end_time":"2022-02-23T10:03:08.099437","exception":false,"start_time":"2022-02-23T10:03:08.078031","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:19.396835Z","iopub.execute_input":"2024-01-12T22:49:19.397061Z","iopub.status.idle":"2024-01-12T22:49:19.404344Z","shell.execute_reply.started":"2024-01-12T22:49:19.397034Z","shell.execute_reply":"2024-01-12T22:49:19.403556Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## **Datasets**\nThe data is labelled by the name, so we load images and label while calling '__getitem__'","metadata":{"papermill":{"duration":0.012739,"end_time":"2022-02-23T10:03:08.125181","exception":false,"start_time":"2022-02-23T10:03:08.112442","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label\n\n","metadata":{"papermill":{"duration":0.023022,"end_time":"2022-02-23T10:03:08.160912","exception":false,"start_time":"2022-02-23T10:03:08.13789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:19.405550Z","iopub.execute_input":"2024-01-12T22:49:19.405730Z","iopub.status.idle":"2024-01-12T22:49:19.419954Z","shell.execute_reply.started":"2024-01-12T22:49:19.405707Z","shell.execute_reply":"2024-01-12T22:49:19.419290Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn_layer1=nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  \n            nn.BatchNorm2d(64),\n            \n        )\n        \n        self.cnn_layer2=nn.Sequential(\n            nn.Conv2d(64, 64, 3, 1, 1), \n            nn.BatchNorm2d(64),\n            \n        )\n        \n        self.cnn_layer3=nn.Sequential(\n            nn.Conv2d(64, 128, 3, 1, 1), \n            nn.BatchNorm2d(128),\n           \n        )\n        \n        self.cnn_layer4=nn.Sequential(\n            nn.Conv2d(128,128, 3, 1, 1), \n            nn.BatchNorm2d(128),\n           \n        )\n        \n        self.cnn_layer5=nn.Sequential(\n            nn.Conv2d(128, 256, 3, 1, 1), \n            nn.BatchNorm2d(256),\n           \n        )\n        self.cnn_layer6=nn.Sequential(\n            nn.Conv2d(256, 256, 3, 1, 1), \n            nn.BatchNorm2d(256),\n           \n        )\n        self.relu=nn.Sequential(\n            nn.ReLU(),\n        )\n        \n        self.maxpool=nn.Sequential(\n            nn.MaxPool2d(2,2,0),\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(256*8*8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(256, 11)\n        )\n\n    def forward(self, x):\n        x1=self.cnn_layer1(x)\n        x1=self.relu(x1)\n        x1=self.maxpool(x1)\n        residual=x1\n        x2=self.cnn_layer2(x1)\n        x2=self.relu(residual+x2)\n        x2=self.maxpool(x2)\n        x3=self.cnn_layer3(x2)\n        x3=self.relu(x3)\n        x3=self.maxpool(x3)\n        residual=x3\n        x4=self.cnn_layer4(x3)\n        x4=self.relu(residual+x4)\n        x5=self.cnn_layer5(x4)\n        x5=self.relu(x5)\n        residual=x5\n        x6=self.cnn_layer6(x5)\n        x6=self.relu(residual+x6)\n        x6=self.maxpool(x6)\n        xout=x6.flatten(1)\n        return self.fc(xout)\n        \n        \n        \n        ","metadata":{"papermill":{"duration":0.0258,"end_time":"2022-02-23T10:03:08.199437","exception":false,"start_time":"2022-02-23T10:03:08.173637","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:19.421059Z","iopub.execute_input":"2024-01-12T22:49:19.421302Z","iopub.status.idle":"2024-01-12T22:49:19.438339Z","shell.execute_reply.started":"2024-01-12T22:49:19.421275Z","shell.execute_reply":"2024-01-12T22:49:19.437615Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n_dataset_dir = \"../input/ml2022spring-hw3b/food11\"\n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = FoodDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)","metadata":{"papermill":{"duration":0.054295,"end_time":"2022-02-23T10:03:08.266338","exception":false,"start_time":"2022-02-23T10:03:08.212043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:19.439553Z","iopub.execute_input":"2024-01-12T22:49:19.439895Z","iopub.status.idle":"2024-01-12T22:49:19.492845Z","shell.execute_reply.started":"2024-01-12T22:49:19.439859Z","shell.execute_reply":"2024-01-12T22:49:19.492190Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"One ../input/ml2022spring-hw3b/food11/training sample ../input/ml2022spring-hw3b/food11/training/0_0.jpg\nOne ../input/ml2022spring-hw3b/food11/validation sample ../input/ml2022spring-hw3b/food11/validation/0_0.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"test_set = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loaders = []\nfor i in range(5):\n    test_set_i = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=train_tfm)\n    test_loader_i = DataLoader(test_set_i, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loaders.append(test_loader_i)\n    ","metadata":{"papermill":{"duration":0.493644,"end_time":"2022-02-23T19:10:19.985992","exception":false,"start_time":"2022-02-23T19:10:19.492348","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing and generate prediction CSV","metadata":{"papermill":{"duration":0.498773,"end_time":"2022-02-23T19:10:20.961802","exception":false,"start_time":"2022-02-23T19:10:20.463029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# The number of training epochs and patience.\nn_epochs = 100\npatience = 20 # If no improvement in 'patience' epochs, early stop\n\n# Initialize a model, and put it on the device specified.\nmodel = Classifier().to(device)\n\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5) \n\n# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break","metadata":{"papermill":{"duration":32830.720158,"end_time":"2022-02-23T19:10:19.001001","exception":false,"start_time":"2022-02-23T10:03:08.280843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-12T22:49:19.494343Z","iopub.execute_input":"2024-01-12T22:49:19.494861Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34cf601c23a043c887eaca930a0ffaab"}},"metadata":{}},{"name":"stdout","text":"[ Train | 001/100 ] loss = 2.25479, acc = 0.20236\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e635490e5f41ccaacb25a112651678"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 001/100 ] loss = 2.03321, acc = 0.27590\n[ Valid | 001/100 ] loss = 2.03321, acc = 0.27590 -> best\nBest model found at epoch 0, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7bc5726445d425dbbd63bb67b30263d"}},"metadata":{}},{"name":"stdout","text":"[ Train | 002/100 ] loss = 2.05855, acc = 0.27196\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d40df71daa0945fb979c6c77eb73bc57"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 002/100 ] loss = 1.91701, acc = 0.31940\n[ Valid | 002/100 ] loss = 1.91701, acc = 0.31940 -> best\nBest model found at epoch 1, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dcae5b56b8941fd997b6101aa6f6def"}},"metadata":{}},{"name":"stdout","text":"[ Train | 003/100 ] loss = 1.99746, acc = 0.29272\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecbc3e81dc746aba1fced55970348c5"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 003/100 ] loss = 1.84925, acc = 0.33745\n[ Valid | 003/100 ] loss = 1.84925, acc = 0.33745 -> best\nBest model found at epoch 2, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff30b38bbe3b44139d651b1e9c50f7b9"}},"metadata":{}},{"name":"stdout","text":"[ Train | 004/100 ] loss = 1.94233, acc = 0.31591\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f7ebb3376440ef854a389bf9a1ae2b"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 004/100 ] loss = 1.90273, acc = 0.33175\n[ Valid | 004/100 ] loss = 1.90273, acc = 0.33175\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c57bf6bba04a1c8332f75d8f071325"}},"metadata":{}},{"name":"stdout","text":"[ Train | 005/100 ] loss = 1.88175, acc = 0.33772\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab14da8f7b1464fae7e2b5332239667"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 005/100 ] loss = 1.76114, acc = 0.38155\n[ Valid | 005/100 ] loss = 1.76114, acc = 0.38155 -> best\nBest model found at epoch 4, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa0dab97005e4a709f06f3de179073cb"}},"metadata":{}},{"name":"stdout","text":"[ Train | 006/100 ] loss = 1.82895, acc = 0.36147\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9e186fbaa2c42a6a3405c623e52a8bb"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 006/100 ] loss = 1.68152, acc = 0.41386\n[ Valid | 006/100 ] loss = 1.68152, acc = 0.41386 -> best\nBest model found at epoch 5, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1debeba9ca8c44bebd8e6f80ff7a3db0"}},"metadata":{}},{"name":"stdout","text":"[ Train | 007/100 ] loss = 1.80170, acc = 0.36506\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d1bbbeac7447669f5e6da026f0ed69"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 007/100 ] loss = 1.66125, acc = 0.42597\n[ Valid | 007/100 ] loss = 1.66125, acc = 0.42597 -> best\nBest model found at epoch 6, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa256da55f8c4bf6878105067b7e0b06"}},"metadata":{}},{"name":"stdout","text":"[ Train | 008/100 ] loss = 1.74977, acc = 0.38060\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df84b190d5b7408491ddc437f0cd2da7"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 008/100 ] loss = 1.61227, acc = 0.43555\n[ Valid | 008/100 ] loss = 1.61227, acc = 0.43555 -> best\nBest model found at epoch 7, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c6d2771cae4227941168a9426cf11c"}},"metadata":{}},{"name":"stdout","text":"[ Train | 009/100 ] loss = 1.70450, acc = 0.40653\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594e3506508841639fe0f11422b3813f"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 009/100 ] loss = 1.62648, acc = 0.43430\n[ Valid | 009/100 ] loss = 1.62648, acc = 0.43430\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11df97721d44a7d9266eb42fb40a282"}},"metadata":{}},{"name":"stdout","text":"[ Train | 010/100 ] loss = 1.67057, acc = 0.41649\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e1f5ddb7994ad4b5a188d996e2a0af"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 010/100 ] loss = 1.69983, acc = 0.41831\n[ Valid | 010/100 ] loss = 1.69983, acc = 0.41831\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3309874261d43a19f8bc2831d6649a3"}},"metadata":{}},{"name":"stdout","text":"[ Train | 011/100 ] loss = 1.64048, acc = 0.42446\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1888d6b626b64978a3f58f8df4802501"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 011/100 ] loss = 1.54266, acc = 0.48419\n[ Valid | 011/100 ] loss = 1.54266, acc = 0.48419 -> best\nBest model found at epoch 10, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f3b629f2c4488d8a6b23a012d594a0"}},"metadata":{}},{"name":"stdout","text":"[ Train | 012/100 ] loss = 1.61856, acc = 0.43317\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cacb5591954248dfa46677c55959e201"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 012/100 ] loss = 1.60933, acc = 0.46018\n[ Valid | 012/100 ] loss = 1.60933, acc = 0.46018\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f593842da4941f6a5c0769a9ef22585"}},"metadata":{}},{"name":"stdout","text":"[ Train | 013/100 ] loss = 1.58970, acc = 0.44770\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48a5aaab1e344e6b0491cfab66ea4b2"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 013/100 ] loss = 1.46196, acc = 0.50369\n[ Valid | 013/100 ] loss = 1.46196, acc = 0.50369 -> best\nBest model found at epoch 12, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1248ed95cc147cb9fcb56c9dbd4cedb"}},"metadata":{}},{"name":"stdout","text":"[ Train | 014/100 ] loss = 1.56991, acc = 0.45288\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1915f61791cb47cdbdb9d5bc47915e81"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 014/100 ] loss = 1.42826, acc = 0.50550\n[ Valid | 014/100 ] loss = 1.42826, acc = 0.50550 -> best\nBest model found at epoch 13, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d900daab62ed4e42af8929a009f5809d"}},"metadata":{}},{"name":"stdout","text":"[ Train | 015/100 ] loss = 1.52684, acc = 0.46855\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f8641916d70428caf6e0873213670df"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 015/100 ] loss = 1.53435, acc = 0.47917\n[ Valid | 015/100 ] loss = 1.53435, acc = 0.47917\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f2e61cd7d74db59b63da7eab7344b1"}},"metadata":{}},{"name":"stdout","text":"[ Train | 016/100 ] loss = 1.52479, acc = 0.47444\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784bca699ecb4b63a43df7df1d86650e"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 016/100 ] loss = 1.40296, acc = 0.51757\n[ Valid | 016/100 ] loss = 1.40296, acc = 0.51757 -> best\nBest model found at epoch 15, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4d239ceb82433cb9d205873079b909"}},"metadata":{}},{"name":"stdout","text":"[ Train | 017/100 ] loss = 1.48902, acc = 0.48496\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458d1beedc114488bde9f83cfbc1d691"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 017/100 ] loss = 1.44315, acc = 0.51494\n[ Valid | 017/100 ] loss = 1.44315, acc = 0.51494\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/155 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3988daabd9aa4368b06c2e7eedd7ec5f"}},"metadata":{}},{"name":"stdout","text":"[ Train | 018/100 ] loss = 1.46386, acc = 0.48746\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a4e1449c364a23bfd5272d67b31dd4"}},"metadata":{}}]},{"cell_type":"code","source":"model_best = Classifier().to(device)\nmodel_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\nmodel_best.eval()\npreds\nwith torch.no_grad():\n    for data,_ in test_loader:\n        test_pred = model_best(data.to(device))\n        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n        prediction += test_label.squeeze().tolist()","metadata":{"papermill":{"duration":49.157727,"end_time":"2022-02-23T19:11:10.61523","exception":false,"start_time":"2022-02-23T19:10:21.457503","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"papermill":{"duration":0.554276,"end_time":"2022-02-23T19:11:11.870035","exception":false,"start_time":"2022-02-23T19:11:11.315759","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}